'# Basis function regression

-- Conjugate gradients solver
def solve (m:Type)?-> : m=>m=>Real -> m=>Real -> m=>Real =
  \mat b.
    x0 = for i:m. 0.0
    ax = mvp mat x0
    r0 = for i. b.i - ax.i
    (xOut, _, _) = fold (x0, r0, r0) $
       \s:m (x, r, p).
         ap = mvp mat p
         alpha = vdot r r / vdot p ap
         x' = for i. x.i + alpha * p.i
         r' = for i. r.i - alpha * ap.i
         beta = vdot r' r' / (vdot r r + 0.000001)
         p' = for i. r'.i + beta * p.i
         (x', r', p')
    xOut

'Make some synthetic data

Nx = Fin 100
noise = 0.1
(k1, k2) = splitKey (newKey 0)

trueFun : Real -> Real =
  \x. x + sin (5.0 * x)

xs : Nx=>Real = for i. rand (ixkey k1 i)
ys : Nx=>Real = for i. trueFun xs.i + noise * randn (ixkey k2 i)

:plot zip xs ys
> <graphical output>

'Implement basis function regression

regress : (Real -> d=>Real) -> n=>Real -> n=>Real -> d=>Real =
  \featurize xRaw y.
    x = map featurize xRaw
    xT = transpose x
    solve (mmp xT x) (mvp xT y)

'Fit a third-order polynomial

poly : Real -> d=>Real =
  \x. for i. pow x (i2r (ordinal i))

params : (Fin 4)=>Real = regress poly xs ys

predict : Real -> Real =
  \x. vdot params (poly x)

:plot
   xsTest = linspace (Fin 100) 0.0 1.0
   zip xsTest (map predict xsTest)
> <graphical output>

'RMS error

rmsErr : n=>Real -> n=>Real -> Real =
  \truth pred. sqrt $ mean for i. sq (pred.i - truth.i)

:p rmsErr ys (map predict xs)
> 9.46494e-2
